Making a larger ai model (let’s say Qwen3 235b) constantly answer a diverse set of questions to train (a) smaller model(s) (let’s say Qwen3 0.6b, 14b, etc.).

This is extremely useful, and definitely isn’t bad — without distillation, you would need 300gbs of vram run any mainstream model.
